{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28bd008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laure\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pjs-8TwGqzJm-py3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from torch.utils.data import Dataset\n",
    "from pjs.dataset import Data\n",
    "import pandas as pd\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37568784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-16 00:46:00.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\bigger_number.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:00.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\first_alphabetically.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:00.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\first_alphabetically_consecutive_first_letter.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:00.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\first_alphabetically_different_first_letter.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:00.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\first_alphabetically_far_first_letter.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:00.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\first_alphabetically_same_first_letter.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:00.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\first_letter.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:00.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\first_word.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:00.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\homophones.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:00.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\last_letter.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:00.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\last_word.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:00.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\least_associated_word.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:00.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\less_letters.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\less_letters_length_diff_1.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\less_letters_length_diff_3plus.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\more_letters.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.137\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\more_letters_length_diff_1.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\more_letters_length_diff_3plus.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\most_associated_word.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\rhyming_word.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\rhyming_word_orthographically_different.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\rhyming_word_orthographically_similar.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\smaller_number.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\word_after.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36mcollect_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mSuccessfully loaded 'examples' from: ..\\data\\LMentry\\word_before.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data = Data(data_path=\"../data/LMentry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55027e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-16 00:46:01.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for bigger_number.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for first_alphabetically.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for first_alphabetically_consecutive_first_letter.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for first_alphabetically_different_first_letter.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for first_alphabetically_far_first_letter.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for first_alphabetically_same_first_letter.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for first_letter.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for first_word.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for homophones.json: 1920 train / 480 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for last_letter.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for last_word.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for least_associated_word.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for less_letters.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for less_letters_length_diff_1.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for less_letters_length_diff_3plus.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for more_letters.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for more_letters_length_diff_1.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for more_letters_length_diff_3plus.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for most_associated_word.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for rhyming_word.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for rhyming_word_orthographically_different.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.250\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for rhyming_word_orthographically_similar.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for smaller_number.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for word_after.json: 2400 train / 600 test rows\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msplit\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSplit for word_before.json: 2400 train / 600 test rows\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data.split(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87160225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-16 00:46:01.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\bigger_number_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\bigger_number_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\first_alphabetically_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\first_alphabetically_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\first_alphabetically_consecutive_first_letter_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\first_alphabetically_consecutive_first_letter_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.580\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\first_alphabetically_different_first_letter_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\first_alphabetically_different_first_letter_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\first_alphabetically_far_first_letter_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\first_alphabetically_far_first_letter_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\first_alphabetically_same_first_letter_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\first_alphabetically_same_first_letter_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\first_letter_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\first_letter_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\first_word_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\first_word_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\homophones_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\homophones_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\last_letter_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\last_letter_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\last_word_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\last_word_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\least_associated_word_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\least_associated_word_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\less_letters_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\less_letters_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\less_letters_length_diff_1_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\less_letters_length_diff_1_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\less_letters_length_diff_3plus_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\less_letters_length_diff_3plus_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\more_letters_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\more_letters_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\more_letters_length_diff_1_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\more_letters_length_diff_1_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\more_letters_length_diff_3plus_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\more_letters_length_diff_3plus_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\most_associated_word_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\most_associated_word_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\rhyming_word_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\rhyming_word_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\rhyming_word_orthographically_different_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\rhyming_word_orthographically_different_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\rhyming_word_orthographically_similar_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\rhyming_word_orthographically_similar_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\smaller_number_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\smaller_number_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\word_after_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\word_after_test.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved train set to ../data/dataset_splits\\split_20250416_004601\\word_before_train.json\u001b[0m\n",
      "\u001b[32m2025-04-16 00:46:01.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpjs.dataset\u001b[0m:\u001b[36msave_dataset_state\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mSaved test set to ../data/dataset_splits\\split_20250416_004601\\word_before_test.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data.save_dataset_state(\"../data/dataset_splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69b9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"phonemetransformers/GPT2-85M-CHAR-TXT\"\n",
    "\n",
    "## Macht W's statt leerzeichen, vielleicht lieber BABYLM-TOKENIZER-CHAR-TXT-SPACELESS ?\n",
    "tokenizer = AutoTokenizer.from_pretrained('phonemetransformers/babble-tokenizers', subfolder='BABYLM-TOKENIZER-CHAR-TXT')\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37277ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-16 00:46:04.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1m['q', ':', 'W', 'i', 'n', 'W', 'a', 'n', 'W', 'a', 'l', 'p', 'h', 'a', 'b', 'e', 't', 'i', 'c', 'a', 'l', 'W', 'o', 'r', 'd', 'e', 'r', ',', 'W', 'w', 'h', 'i', 'c', 'h', 'W', 'w', 'o', 'r', 'd', 'W', 'c', 'o', 'm', 'e', 's', 'W', 'f', 'i', 'r', 's', 't', ',', 'W', '\"', 'b', 'l', 'a', 'n', 'k', 'e', 't', '\"', 'W', 'o', 'r', 'W', '\"', 'd', 'i', 'a', 'r', 'y', '\"', '?', 'W', 'a', ':']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Q: In an alphabetical order, which word comes first, \\\"blanket\\\" or \\\"diary\\\"? A:\"\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "logger.info(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccaf117b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "encodings = data.get_tokenised_train_split(tokenizer)\n",
    "\n",
    "input_encodings = encodings[\"first_word.json\"][0]\n",
    "label_encodings = encodings[\"first_word.json\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb164587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laure\\AppData\\Local\\Temp\\ipykernel_20996\\1051673243.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 16:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.677600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.380300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.368900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=900, training_loss=0.4162959416707357, metrics={'train_runtime': 986.6348, 'train_samples_per_second': 7.298, 'train_steps_per_second': 0.912, 'total_flos': 422558208000000.0, 'train_loss': 0.4162959416707357, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            key: val[idx] for key, val in self.encodings.items()\n",
    "        }\n",
    "\n",
    "encodings = {\n",
    "    \"input_ids\": input_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "    \"labels\": label_encodings[\"input_ids\"],\n",
    "}\n",
    "\n",
    "dataset = CustomDataset(encodings)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../data/results\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7d7b5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-16 01:02:46.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1m                                                 input output\n",
      "0    Q: In the sentence \"John was lots more obnoxio...   John\n",
      "1    Q: In the sentence \"A boy hit the ball\", what ...      A\n",
      "2    Q: In the sentence \"I remembered having kissed...      I\n",
      "3    Q: What is the first word of the sentence \"I b...      I\n",
      "4    Write the first word of the sentence \"John wil...   John\n",
      "..                                                 ...    ...\n",
      "595  Q: What is the first word of the sentence \"Sus...  Susan\n",
      "596  Write the first word of the sentence \"John mad...   John\n",
      "597  Q: What is the first word of the sentence \"We ...     We\n",
      "598  Q: In the sentence \"John scratched his arm and...   John\n",
      "599  Q: In the sentence \"The brave are not afraid t...    The\n",
      "\n",
      "[600 rows x 2 columns]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_data = data.split_data[\"first_word.json\"][\"test\"]\n",
    "\n",
    "logger.info(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eeec6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: In the sentence \"John was lots more obnoxious than Fred\", what is the first word? A: John\n",
      "Input: Q: In the sentence \"John was lots more obnoxious than Fred\", what is the first word? A:\n",
      "Expected: John\n",
      "Predicted: q: in the sentence \"john was lots more obnoxious than fred\", what is the first word? a: hat is the first word? a: is the first woringlerififingfisfin\n",
      "---\n",
      "Q: In the sentence \"A boy hit the ball\", what is the first word? A: A\n",
      "Input: Q: In the sentence \"A boy hit the ball\", what is the first word? A:\n",
      "Expected: A\n",
      "Predicted: q: in the sentence \"a boy hit the ball\", what is the first word? a: ell, i think that was a good idea.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for index, row in test_data.head(2).iterrows():\n",
    "    print(row[\"input\"], row[\"output\"])\n",
    "    input = row[\"input\"]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "    input,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=200,\n",
    "    return_attention_mask=True\n",
    "    )\n",
    "    \n",
    "    # Modell generiert Fortsetzung\n",
    "    outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=150,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Ausgabe-Text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "    output_text = output_text.replace(\"UTT_BOUNDARY\", \"\")\n",
    "    output_text = output_text.replace(\" \", \"\")\n",
    "    output_text = output_text.replace(\"W\", \" \")\n",
    "    \n",
    "    print(f\"Input: {row[\"input\"]}\")\n",
    "    print(f\"Expected: {row['output']}\")\n",
    "    print(f\"Predicted: {output_text}\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pjs-8TwGqzJm-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
